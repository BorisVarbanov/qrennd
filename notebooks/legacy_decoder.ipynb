{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import tee\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize as optim\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.compat.v1.nn.rnn_cell import (BasicLSTMCell, DropoutWrapper,\n",
    "                                              MultiRNNCell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_single_param(t, p_logical):\n",
    "    \"\"\"Function that models exponential (fidelity) decay controlled\n",
    "        by a single parameter, the decay rate.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    t -- Time, a positive float or integer.\n",
    "    p_logical -- Decay rate, a positive float number.\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    fidelity -- The logical fidelity, a float number between 0 and 1.\n",
    "    \"\"\"\n",
    "    fidelity = (1 + (1 - 2 * p_logical) ** t) / 2.0\n",
    "    return fidelity\n",
    "\n",
    "\n",
    "def calc_plog(data, p0=(0.001,), bounds=((0.00001), (0.2))):\n",
    "    \"\"\"Function to calculate the logical error rate.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    data -- List of lists.\n",
    "    p0 -- A starting point for the fits.\n",
    "    bounds -- Boundaries for the fitting, tuple of tuples of floats.\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    plog -- The logical error rate, a float number.\n",
    "    \"\"\"\n",
    "\n",
    "    # It is possible that the batch does not contain fidelities\n",
    "    # for all steps, hence we need a list with all steps for which\n",
    "    # predictions exist (we call it 'steps').\n",
    "    steps, data_nonzero = [], []\n",
    "    fids = []\n",
    "\n",
    "    # In the following we assume that the first step is s = 1.\n",
    "    for s in range(1, len(data) + 1):\n",
    "        dat = data[s - 1]\n",
    "        if len(dat) != 0:\n",
    "            # Non-trivial data points\n",
    "            steps.append(s)\n",
    "            data_nonzero.append(dat)\n",
    "            # Fidelities\n",
    "            fids.append(np.mean(dat))\n",
    "\n",
    "    # We fit a decay curve to the non-tivial data.\n",
    "    popt, pcov = optim.curve_fit(decay_single_param, steps, fids, p0, bounds=bounds)\n",
    "    plog = popt[0]\n",
    "    return plog\n",
    "\n",
    "\n",
    "class Decoder:\n",
    "\n",
    "    \"\"\"This class describes a neural network which consist of several\n",
    "    long-short term memory (LSTM) layers followed by two heads consitings of\n",
    "    feed forward layers. It is designed for quantum error correction on\n",
    "    stabilizer codes, such as the surface codes or the color code.\n",
    "\n",
    "    The __init__ function takes the following parameters:\n",
    "\n",
    "    code_distance -- The distance of the quantum error correction code.\n",
    "\n",
    "    dim_syndr -- The dimension of the syndrome increments, i.e. the number\n",
    "                 of stabilizers. In a surface 17 code for example, it would be 8.\n",
    "\n",
    "    dim_fsyndr -- The dimension of the final syndrome increment (which one can\n",
    "                  calculate from the readout of the data qubits).\n",
    "\n",
    "    lstm_iss -- A list containing the sizes of the internal states of the\n",
    "                LSTM layers.\n",
    "\n",
    "    ff_layer_sizes -- A list containing the number of neurons per layer of\n",
    "                      the feedforward networks.\n",
    "\n",
    "    checkpoint_path -- A path to which the network gets saved to, both for\n",
    "                       intermediate and final instances.\n",
    "\n",
    "    keep_prob -- Dropout regularization of the feedforward and LSTM layers. This\n",
    "                 parameter controlls the percentage of how many entries of the\n",
    "                 output vectors are NOT set to zero (on average) during training.\n",
    "\n",
    "    aux_loss_factor -- This scalar parameters controls how much the cost function\n",
    "                       weighs the auxillary head of the network. If it is 1,\n",
    "                       then both heads are weighted equally.\n",
    "\n",
    "    l2_prefactor -- Prefactor for L2 weight regularization of the feed forward\n",
    "                    layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # # # Initialization functions # # #\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        code_distance,\n",
    "        dim_syndr,\n",
    "        dim_fsyndr,\n",
    "        lstm_iss,\n",
    "        ff_layer_sizes,\n",
    "        checkpoint_path,\n",
    "        keep_prob=1,\n",
    "        aux_loss_factor=1,\n",
    "        l2_prefactor=0,\n",
    "    ):\n",
    "        \"\"\"This function initializes an instance of the Decoder class. Its inputs\n",
    "        are described in the class documentation above.\"\"\"\n",
    "\n",
    "        # Factor by which the loss of the auxillary head is multiplied\n",
    "        self.aux_loss_factor = aux_loss_factor\n",
    "\n",
    "        # Directory to save the network and feedback (must exist)\n",
    "        self.cp_path = checkpoint_path\n",
    "\n",
    "        # Initialize input related variables\n",
    "        self._init_data_params(code_distance, dim_syndr, dim_fsyndr)\n",
    "\n",
    "        # Set parameters that define the network size\n",
    "        self._init_network_params(lstm_iss, ff_layer_sizes)\n",
    "\n",
    "        # Set parameters that control the training\n",
    "        self._init_training_params(keep_prob, l2_prefactor)\n",
    "\n",
    "        # Build the graph\n",
    "        self._init_graph()\n",
    "\n",
    "    def _init_data_params(self, code_distance, dim_syndr, dim_fsyndr):\n",
    "        \"\"\"A subfunction of __init__, setting variables related to the input\n",
    "        data, the input variables are described in class documentation above.\n",
    "        \"\"\"\n",
    "        self.code_dist = code_distance\n",
    "        self.dim_syndr = dim_syndr\n",
    "        self.dim_fsyndr = dim_fsyndr\n",
    "\n",
    "    def _init_network_params(self, lstm_iss, ff_layer_sizes):\n",
    "        \"\"\"A subfunction of __init__, setting the variables that define the\n",
    "        network size, the input variables are described in class documentation\n",
    "        above.\"\"\"\n",
    "\n",
    "        # LSTM layers' parameters\n",
    "        self.lstm_iss = lstm_iss\n",
    "\n",
    "        # Feedfoward layers' parameters\n",
    "        self.ff_aux_lr_s = [lstm_iss[-1]] + ff_layer_sizes + [1]\n",
    "        self.ff_lr_s = [lstm_iss[-1] + self.dim_fsyndr] + ff_layer_sizes + [1]\n",
    "\n",
    "    def _init_training_params(self, keep_prob, l2_prefactor):\n",
    "        \"\"\"A subfunction of __init__, setting the variables that define the\n",
    "        training procedure, the input variables are described in class\n",
    "        documentation above.\"\"\"\n",
    "\n",
    "        # Dropout of the outputs in the LSTM network\n",
    "        self.kp = keep_prob\n",
    "\n",
    "        # Prefactor for L2 weight regularization (feedforward layers only)\n",
    "        self.l2_prefact = l2_prefactor\n",
    "\n",
    "        # Variables to keep track of training process\n",
    "        self.total_trained_epochs = 0\n",
    "        self.total_trained_batches = 0\n",
    "\n",
    "    def _init_graph(self):\n",
    "        \"\"\"A subfunction of __init__, defining the graph, initializing the\n",
    "        tensorflow variables, and the optimizer.\"\"\"\n",
    "\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self._init_network_variables()\n",
    "            self._init_network_functions()\n",
    "\n",
    "    def _init_network_variables(self):\n",
    "        \"\"\"A subfunction of _init_graph, defining the tensorflow placeholders,\n",
    "        weights and biases of the feed forward networks, and corresponding\n",
    "        saver instances.\"\"\"\n",
    "\n",
    "        # Here we defind placeholders ...\n",
    "        with tf.variable_scope(\"input\"):\n",
    "            # ... for the input of the syndrome increments\n",
    "            self.x = tf.placeholder(\n",
    "                tf.float32, [None, None, self.dim_syndr], name=\"x_input\"\n",
    "            )\n",
    "            # ... for the input of the final syndrome increments\n",
    "            self.fx = tf.placeholder(\n",
    "                tf.float32, [None, self.dim_fsyndr], name=\"fx_input\"\n",
    "            )\n",
    "            # ... for the parity of the bitflips\n",
    "            self.y = tf.placeholder(tf.float32, [None, 1], name=\"y_input\")\n",
    "            # ... for the number of stabilizer measurement cycles in a sequence\n",
    "            self.length = tf.placeholder(tf.int32, [None], name=\"length_input\")\n",
    "\n",
    "        with tf.variable_scope(\"training_parameters\"):\n",
    "            # ... for the learning rate\n",
    "            self.lr = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "            # ... for the weighing of the auxillary head\n",
    "            self.alf = tf.placeholder(tf.float32, name=\"aux_loss_factor\")\n",
    "\n",
    "            # ... for the dropout (keep probabilities)\n",
    "            self.lstm_kp = tf.placeholder(tf.float32, name=\"lstm_keep_probability\")\n",
    "            self.ff_kp = tf.placeholder(tf.float32, name=\"ff_keep_probability\")\n",
    "\n",
    "        with tf.variable_scope(\"summary_placeholders\"):\n",
    "            # ... for the tensorboard summaries\n",
    "            self.plog = tf.placeholder(tf.float32, name=\"plog_train\")\n",
    "            self.plog_aux = tf.placeholder(tf.float32, name=\"plog_aux_train\")\n",
    "            self.tot_cost = tf.placeholder(tf.float32, name=\"tot_cost\")\n",
    "\n",
    "    def _init_network_functions(self):\n",
    "        \"\"\"A subfunction of _init_graph, defining all the main functions of the\n",
    "        graph.\"\"\"\n",
    "\n",
    "        with tf.variable_scope(\"NET\"):\n",
    "            # Gathering the network outputs (logits)\n",
    "            out, out_aux = self.network(self.x, self.fx)\n",
    "            logits = tf.reshape(out, [-1, 1])\n",
    "            logits_aux = tf.reshape(out_aux, [-1, 1])\n",
    "\n",
    "        with tf.variable_scope(\"prediction\"):\n",
    "            # Calculating the probabilities that the parity of bitflips is odd\n",
    "            self.predictions = tf.nn.sigmoid(logits)\n",
    "            self.predictions_aux = tf.nn.sigmoid(logits_aux)\n",
    "            p = tf.nn.sigmoid(logits)\n",
    "            p_aux = tf.nn.sigmoid(logits_aux)\n",
    "\n",
    "            # Adding the network outputs and predictions to the summary\n",
    "            tf.summary.histogram(\"logits\", clip(logits), collections=[\"feedback\"])\n",
    "            tf.summary.histogram(\"p\", p, collections=[\"feedback\"])\n",
    "            tf.summary.histogram(\n",
    "                \"logits_aux\", clip(logits_aux), collections=[\"feedback\"]\n",
    "            )\n",
    "            tf.summary.histogram(\"p_aux\", p_aux, collections=[\"feedback\"])\n",
    "\n",
    "        with tf.variable_scope(\"cost\"):\n",
    "            # Calculate the cross entropy for the main head\n",
    "            cross_entropy = tf.losses.sigmoid_cross_entropy(\n",
    "                logits=logits, multi_class_labels=self.y\n",
    "            )\n",
    "            self.cost_crossentro = tf.reduce_sum(cross_entropy)\n",
    "\n",
    "            # Calculate the cross entropy for the auxillary head\n",
    "            cross_entropy_aux = tf.losses.sigmoid_cross_entropy(\n",
    "                logits=logits_aux, multi_class_labels=self.y\n",
    "            )\n",
    "            self.cost_crossentro_aux = tf.reduce_sum(cross_entropy_aux)\n",
    "\n",
    "            # Calculate the L2 norm of the feed forward networks' weights\n",
    "            # to do weight regularization (not for the biases)\n",
    "            col_ff = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"NET/NET_FF\")\n",
    "            weights_l = []\n",
    "            for el in col_ff:\n",
    "                if \"weights\" in el.name:\n",
    "                    weights_l.append(el)\n",
    "            self.l2_loss = (\n",
    "                tf.add_n([tf.nn.l2_loss(v) for v in weights_l]) * self.l2_prefact\n",
    "            )\n",
    "            # Print some feedback\n",
    "            print(\"added the following variables to L2 weight regularization term:\")\n",
    "            for var in weights_l:\n",
    "                print(var.name)\n",
    "\n",
    "            # The total cost function is the sum of the two cross-entropies plus\n",
    "            # the weight regularization term.\n",
    "            self.cost = (\n",
    "                self.cost_crossentro\n",
    "                + self.l2_loss\n",
    "                + self.aux_loss_factor * self.cost_crossentro_aux\n",
    "            )\n",
    "\n",
    "        # Writing the costs and logical error rates to the feedback summary.\n",
    "        with tf.variable_scope(\"feedback\"):\n",
    "            # costs\n",
    "            tf.summary.scalar(\n",
    "                \"crossentropy\", self.cost_crossentro, collections=[\"feedback\"]\n",
    "            )\n",
    "            tf.summary.scalar(\n",
    "                \"crossentropy_aux\", self.cost_crossentro_aux, collections=[\"feedback\"]\n",
    "            )\n",
    "            tf.summary.scalar(\"l2_loss\", self.l2_loss, collections=[\"feedback\"])\n",
    "            tf.summary.scalar(\"cost\", self.tot_cost, collections=[\"feedback\"])\n",
    "\n",
    "            # logical error rate\n",
    "            tf.summary.scalar(\"logical_error_rate\", self.plog, collections=[\"feedback\"])\n",
    "            tf.summary.scalar(\n",
    "                \"logical_error_rate_aux\", self.plog_aux, collections=[\"feedback\"]\n",
    "            )\n",
    "\n",
    "        # Writing some feedback regarding the nature of the input data\n",
    "        with tf.variable_scope(\"network_parameters\"):\n",
    "            tf.summary.scalar(\n",
    "                \"min_length\", tf.reduce_min(self.length), collections=[\"feedback\"]\n",
    "            )\n",
    "            tf.summary.scalar(\n",
    "                \"max_length\", tf.reduce_max(self.length), collections=[\"feedback\"]\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope(\"optimizer\"):\n",
    "            # Defining the network optimization algorithm\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(\n",
    "                self.cost\n",
    "            )\n",
    "\n",
    "        # Tensorflow saver, to save checkpoints of the network\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # Merge summaries\n",
    "        self.merged_summaries = tf.summary.merge_all(\"network\")\n",
    "        self.merged_summaries_fb = tf.summary.merge_all(\"feedback\")\n",
    "\n",
    "        # Define separate summary writers for training and validation\n",
    "        self.train_writer = tf.summary.FileWriter(\n",
    "            self.cp_path + \"/tensorboard/training\", self.graph\n",
    "        )\n",
    "        self.val_writer = tf.summary.FileWriter(\n",
    "            self.cp_path + \"/tensorboard/validation\", self.graph\n",
    "        )\n",
    "\n",
    "        # Finally, we initialize the network variables\n",
    "        self.initialize_NN = tf.global_variables_initializer()\n",
    "\n",
    "    # # # Functions that define the neural network # # #\n",
    "    def network(self, input_syndr, input_fsyndr):\n",
    "        \"\"\"This function defines the neural network.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        input_syndr -- A placeholder that will later contain the lists with\n",
    "                       syndrome increments.\n",
    "\n",
    "        input_fsyndr -- A placeholder that will later contain the final\n",
    "                        syndrome increments.\n",
    "\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        ff_net -- The neural network with the main evaluation head.\n",
    "\n",
    "        ff_net_aux -- The neural network with the auxillary evaluation head.\n",
    "        \"\"\"\n",
    "\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "        # # # # # # # #  Recurrent part (LSTM) of the network # # # # # # # # #\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "        with tf.variable_scope(\"NET_LSTM\"):\n",
    "            # First, we create LSTM cells with (optional) dropout. Here dropout is\n",
    "            # only applied to the outputs of the layers.\n",
    "            cells = []\n",
    "            for iss in self.lstm_iss:\n",
    "                cell = BasicLSTMCell(iss)\n",
    "                cell = DropoutWrapper(cell=cell, output_keep_prob=self.lstm_kp)\n",
    "                cells.append(cell)\n",
    "\n",
    "            # Next, we combine LSTM cells into a recurrent network.\n",
    "            lstm_cells = MultiRNNCell(cells)\n",
    "            # Since each batch will contain sequences of variying number or\n",
    "            # measurement cycles, we use dynamic_rrn which allows for a variable\n",
    "            # sequence_length.\n",
    "            lstm_out, lstm_states = tf.nn.dynamic_rnn(\n",
    "                lstm_cells,\n",
    "                input_syndr,\n",
    "                sequence_length=self.length,\n",
    "                dtype=tf.float32,\n",
    "                scope=\"dyn_rnn\",\n",
    "            )\n",
    "\n",
    "            # We now select the output of the last LSTM cell after the last cycle, as\n",
    "            # specified by self.length.\n",
    "            last_lstm_out = tf.gather_nd(\n",
    "                lstm_out,\n",
    "                tf.stack([tf.range(tf.shape(lstm_out)[0]), self.length - 1], axis=1),\n",
    "            )\n",
    "            # We store this last output as feedback to be displayed with tensorboard.\n",
    "            tf.summary.histogram(\n",
    "                \"lstm_out\", clip(last_lstm_out), collections=[\"feedback\"]\n",
    "            )\n",
    "            # Finally, we apply a rectified linear activation function to the output.\n",
    "            last_lstm_out = tf.nn.relu(last_lstm_out)\n",
    "\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "        # # # # # # # # # Feedforward part of the network # # # # # # # # # # #\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "        # # # Main evaluation head # # #\n",
    "        with tf.variable_scope(\"NET_FF\"):\n",
    "            # The main evaluation head is a feedforward network with rectified linear\n",
    "            # units. It gets as an input last_lstm_out concatenated with the final\n",
    "            # syndrome increment.\n",
    "            ff_net = tf.concat([last_lstm_out, input_fsyndr], 1)\n",
    "\n",
    "            # Here we define the feedforward network. The output will be a\n",
    "            # single float number. The final output is not subject to an\n",
    "            # activation function.\n",
    "            n_lrs = len(self.ff_lr_s) - 1\n",
    "            for n in range(n_lrs):\n",
    "                if n < n_lrs - 1:\n",
    "                    act_fct = tf.nn.relu\n",
    "                    has_bias = True\n",
    "                else:\n",
    "                    act_fct = tf.identity\n",
    "                    has_bias = False\n",
    "                ff_net = self._make_layer(\n",
    "                    ff_net,\n",
    "                    self.ff_lr_s[n],\n",
    "                    self.ff_lr_s[n + 1],\n",
    "                    act_fct,\n",
    "                    \"layer_\" + str(n),\n",
    "                    has_bias,\n",
    "                )\n",
    "\n",
    "        # # # Auxillary evaluation head # # #\n",
    "        with tf.variable_scope(\"NET_FF_aux\"):\n",
    "            # The auxillary evaluation head is a feedforward network with rectified\n",
    "            # linear units. It gets as an input last_lstm_out. Its purpose is to\n",
    "            # encourage translation invariance in time of the recurrent network.\n",
    "            ff_net_aux = last_lstm_out\n",
    "\n",
    "            # Here we define the feedforward network. The output will be a\n",
    "            # single float number. The final output is not subject to an\n",
    "            # activation function.\n",
    "            n_lrs = len(self.ff_aux_lr_s) - 1\n",
    "            for n in range(n_lrs):\n",
    "                if n < n_lrs - 1:\n",
    "                    act_fct = tf.nn.relu\n",
    "                    has_bias = True\n",
    "                else:\n",
    "                    act_fct = tf.identity\n",
    "                    has_bias = False\n",
    "                ff_net_aux = self._make_layer(\n",
    "                    ff_net_aux,\n",
    "                    self.ff_aux_lr_s[n],\n",
    "                    self.ff_aux_lr_s[n + 1],\n",
    "                    act_fct,\n",
    "                    \"layer_\" + str(n),\n",
    "                    has_bias,\n",
    "                )\n",
    "\n",
    "        return ff_net, ff_net_aux\n",
    "\n",
    "    def _make_layer(self, input_tensor, dim_in, dim_out, act_fct, name, has_bias=True):\n",
    "        \"\"\"This function builds a single layer of a feedforward network.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        input_tensor -- The tensor that will be fed into the layer.\n",
    "\n",
    "        dim_in -- The dimension of the input tensor.\n",
    "\n",
    "        dim_out -- The number of neurons in the layer.\n",
    "\n",
    "        act_fct -- The activation function.\n",
    "\n",
    "        name -- The name of the layer.\n",
    "\n",
    "        has_bias -- If True, the layer will have a bias vector added to\n",
    "                    the neurons (before the activation function is applied).\n",
    "\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        layer_after_dropout -- The layer with dropout.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            # First, initialize the weights and biases\n",
    "            weights = tf.Variable(\n",
    "                tf.random_uniform(\n",
    "                    [dim_in, dim_out],\n",
    "                    minval=-1.0 / np.sqrt(dim_in),\n",
    "                    maxval=+1.0 / np.sqrt(dim_in),\n",
    "                ),\n",
    "                dtype=tf.float32,\n",
    "                name=\"weights\",\n",
    "            )\n",
    "            tf.summary.histogram(\"weights\", clip(weights), collections=[\"network\"])\n",
    "            if has_bias:\n",
    "                biases = tf.Variable(\n",
    "                    tf.constant(0.0, tf.float32, [dim_out]),\n",
    "                    dtype=tf.float32,\n",
    "                    name=\"biases\",\n",
    "                )\n",
    "                tf.summary.histogram(\"biases\", clip(biases), collections=[\"network\"])\n",
    "            # Second, define the layer.\n",
    "            if has_bias:\n",
    "                layer = tf.matmul(input_tensor, weights) + biases\n",
    "            else:\n",
    "                layer = tf.matmul(input_tensor, weights)\n",
    "            # ... and write some feedback\n",
    "            tf.summary.histogram(\"preact\", clip(layer), collections=[\"feedback\"])\n",
    "\n",
    "            # Third, apply the activation function\n",
    "            layer_activations = act_fct(layer, name=\"activations\")\n",
    "            # ... and write some feedback\n",
    "            tf.summary.histogram(\n",
    "                \"act\", clip(layer_activations), collections=[\"feedback\"]\n",
    "            )\n",
    "\n",
    "            # Finally, apply dropout to the output of the layer\n",
    "            layer_after_dropout = tf.nn.dropout(layer_activations, self.ff_kp)\n",
    "        return layer_after_dropout\n",
    "\n",
    "    # # # Starting and ending the session # # #\n",
    "    def start_session(self, gpu_options=None):\n",
    "        \"\"\"This function (re-)initializes the TensorFlow session\"\"\"\n",
    "\n",
    "        # Optionally, GPU options can be specified.\n",
    "        if gpu_options is None:\n",
    "            self.sess = tf.Session(graph=self.graph)\n",
    "        else:\n",
    "            self.sess = tf.Session(\n",
    "                graph=self.graph, config=tf.ConfigProto(gpu_options=gpu_options)\n",
    "            )\n",
    "\n",
    "        # Initialize the TensorFlow session.\n",
    "        self.sess.run(self.initialize_NN)\n",
    "\n",
    "    def end_session(self):\n",
    "        \"\"\"This function ends the current TensorFlow session\"\"\"\n",
    "        self.sess.close()\n",
    "\n",
    "    def feed_dict(self, b_x, b_fx, b_l, b_y, learning_rate, lstm_kp, ff_kp):\n",
    "        \"\"\"This function creates and returns a 'feed dictionary', that\n",
    "        can be passed to the session for network training and evaluation\n",
    "        purposes\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        b_x -- A batch with syndrome increments.\n",
    "\n",
    "        b_fx -- A batch with final syndrome increments.\n",
    "\n",
    "        b_l -- A batch specifying the number of stabilizer measurement\n",
    "               cycles for each batch entry.\n",
    "\n",
    "        b_y -- A batch with the true parities.\n",
    "\n",
    "        learning_rate -- A single float number which sets the learning\n",
    "                         rate of the optimizer.\n",
    "\n",
    "        lstm_kp -- A single float number that sets the keep-rate of the\n",
    "                   dropout after the LSTM layers.\n",
    "\n",
    "        ff_kp -- A single float number that sets the keep-rate of the\n",
    "                 dropout after the feedforward layers.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            self.x: b_x,\n",
    "            self.fx: b_fx,\n",
    "            self.length: b_l,\n",
    "            self.y: b_y,\n",
    "            self.lr: learning_rate,\n",
    "            self.lstm_kp: lstm_kp,\n",
    "            self.ff_kp: ff_kp,\n",
    "        }\n",
    "\n",
    "    # # # Functions for training the network # # #\n",
    "    def train_one_epoch(self, train_batches, learning_rate=0.001):\n",
    "        \"\"\"This function controlls the training of the network for one epoch.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        train_batches -- The batches that consitute one epoch.\n",
    "\n",
    "        learning_rate -- The learning rate of the (Adam) optimizer.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Here we train the network\n",
    "        for b_s, b_x, b_fx, b_l, b_y in train_batches:\n",
    "            fd = self.feed_dict(b_x, b_fx, b_l, b_y, learning_rate, self.kp, self.kp)\n",
    "            self.sess.run(self.optimizer, feed_dict=fd)\n",
    "            self.total_trained_batches += 1\n",
    "        self.total_trained_epochs += 1\n",
    "\n",
    "        # After the training we write some feedback to the summaries\n",
    "        summary = self.sess.run(self.merged_summaries, feed_dict=fd)\n",
    "        self.train_writer.add_summary(summary, self.total_trained_epochs)\n",
    "\n",
    "        # Finally, we save the network\n",
    "        self.save_network(\"model\")\n",
    "\n",
    "    def calc_feedback(self, batches, validation=True):\n",
    "        \"\"\"This function calculates feedback and summaries.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        batches -- The batches over which the feedback is calculated.\n",
    "\n",
    "        validation -- If True, the validation summary writer is used.\n",
    "                      If False, the traning summary writer is used.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # First, we copy the batch generator two times\n",
    "        batches, batches_copy = tee(batches)\n",
    "        batches, batches_copy_aux = tee(batches)\n",
    "\n",
    "        # We use the first copy to calculate the logical error rate for\n",
    "        # the main head ...\n",
    "        plog = calc_plog(self.test_net(batches_copy, auxillary=False))\n",
    "        # ... and the second copy for the auxillary head\n",
    "        plog_aux = calc_plog(self.test_net(batches_copy_aux, auxillary=True))\n",
    "\n",
    "        # We then use the original generator to calculate the cost.\n",
    "        # For practicality, we only use a single batch for that and\n",
    "        # the other feedback\n",
    "        b_s, b_x, b_fx, b_l, b_y = next(batches)\n",
    "        fd = self.feed_dict(b_x, b_fx, b_l, b_y, 0, 1, 1)\n",
    "        cost = self.cost.eval(session=self.sess, feed_dict=fd)\n",
    "\n",
    "        # Finally, we write this information to the summary\n",
    "        fd = {\n",
    "            self.plog: plog,\n",
    "            self.plog_aux: plog_aux,\n",
    "            self.tot_cost: cost,\n",
    "            self.x: b_x,\n",
    "            self.fx: b_fx,\n",
    "            self.length: b_l,\n",
    "            self.y: b_y,\n",
    "            self.lr: 0,\n",
    "            self.lstm_kp: 1,\n",
    "            self.ff_kp: 1,\n",
    "        }\n",
    "        summary_fb = self.sess.run(self.merged_summaries_fb, feed_dict=fd)\n",
    "        if validation:\n",
    "            self.val_writer.add_summary(summary_fb, self.total_trained_epochs)\n",
    "            print(\"logical error rate on validation set is\", round(plog, 4))\n",
    "        else:\n",
    "            self.train_writer.add_summary(summary_fb, self.total_trained_epochs)\n",
    "            print(\"logical error rate on training set is\", round(plog, 4))\n",
    "        return plog\n",
    "\n",
    "    def test_net(self, batches, auxillary=False):\n",
    "        \"\"\"A function that evaluates the network.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        batches -- A generator of batches that will be evaluated.\n",
    "\n",
    "        auxillary -- If True, the auxillary head is used for evaluation.\n",
    "                     If False, the main head is used for evaluation.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        length_list, comp_list = [], []\n",
    "        for b_s, b_x, b_fx, b_l, b_y in batches:\n",
    "            # Get probabilities that a bitflip occured from the network\n",
    "            fd = self.feed_dict(b_x, b_fx, b_l, b_y, 0, 1, 1)\n",
    "            if auxillary:\n",
    "                preds = self.predictions_aux.eval(feed_dict=fd, session=self.sess)\n",
    "            else:\n",
    "                preds = self.predictions.eval(feed_dict=fd, session=self.sess)\n",
    "\n",
    "            # Reshape tensors to 1D\n",
    "            preds = np.reshape(preds, [-1])\n",
    "            b_y = np.reshape(b_y, [-1])\n",
    "\n",
    "            # Make a decision (0 for even, 1 for odd) of the total parity of bitflips\n",
    "            preds = np.around(preds).astype(bool)\n",
    "\n",
    "            # Compare predictions to true results\n",
    "            comp_list += list(np.equal(preds, b_y))\n",
    "\n",
    "            # Update maximum length of time-sequence\n",
    "            length_list += list(b_l)\n",
    "\n",
    "        # Reshape into a list of lists\n",
    "        comparison = [[] for _ in range(max(length_list))]\n",
    "        for n in range(len(comp_list)):\n",
    "            idx = length_list[n] - 1\n",
    "            comparison[idx].append(comp_list[n])\n",
    "\n",
    "        return comparison\n",
    "\n",
    "    # # # Functions to save and restore the network # # #\n",
    "    def save_network(self, fname, with_step=True):\n",
    "        \"\"\"Function to save all variables of the network.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        fname -- The  filename to which the network is saved, note\n",
    "                 that by default the path is the checkpoint_path/model/.\n",
    "\n",
    "        with_step -- If True, the variable \"global_step\" will be set to\n",
    "                     self.total_trained_epochs. This can be useful if the\n",
    "                     training has been interrupted.\n",
    "        \"\"\"\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            saver = tf.train.Saver()\n",
    "            if with_step:\n",
    "                saver.save(\n",
    "                    self.sess,\n",
    "                    self.cp_path + \"model/\" + fname,\n",
    "                    global_step=self.total_trained_epochs,\n",
    "                )\n",
    "            else:\n",
    "                saver.save(self.sess, self.cp_path + \"model/\" + fname)\n",
    "\n",
    "    def load_network(self, fname):\n",
    "        \"\"\"Function to load the network from a checkpoint.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        fname -- is the filename from which the network is loaded, note\n",
    "                 that by default the path is the checkpoint_path/model/.\n",
    "\n",
    "        \"\"\"\n",
    "        with self.graph.as_default():\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(self.sess, self.cp_path + \"model/\" + fname)\n",
    "\n",
    "\n",
    "# # # Functions # # #\n",
    "\n",
    "\n",
    "def clip(var):\n",
    "    \"\"\"Clips elements of a tensor at mean +- 3 standard deviations.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    var -- A tensorflow tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    lin = tf.reshape(var, [-1])\n",
    "    mean, variance = tf.nn.moments(lin, axes=[0])\n",
    "    sigma = tf.sqrt(variance)\n",
    "    cv_min = mean - 3 * sigma\n",
    "    cv_max = mean + 3 * sigma\n",
    "    return tf.clip_by_value(var, cv_min, cv_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # PARAMETERS # # #\n",
    "\n",
    "# Network (parameters for 6-6-6 color code with distance 3)\n",
    "code_distance = 3  # distance of the QEC code\n",
    "dim_syndr = 12  # dimension of the syndrome increment vectors\n",
    "dim_fsyndr = 3  # dimension of the final syndrome increment vectors\n",
    "network_size = 32  # size of the LSTM's internal states and number of the the FF layers' neurons\n",
    "\n",
    "# Network (parameters for 6-6-6 color code with distance 5)\n",
    "# code_distance = 5  # distance of the QEC code\n",
    "# dim_syndr = 36  # dimension of the syndrome increment vectors\n",
    "# dim_fsyndr = 9  # dimension of the final syndrome increment vectors\n",
    "# network_size = 64  # size of the LSTM's internal states and number of the the FF layers' neurons\n",
    "\n",
    "# Network (parameters for 6-6-6 color code with distance 7)\n",
    "# code_distance = 7  # distance of the QEC code\n",
    "# dim_syndr = 72  # dimension of the syndrome increment vectors\n",
    "# dim_fsyndr = 18  # dimension of the final syndrome increment vectors\n",
    "# network_size = 128  # size of the LSTM's internal states and number of the the FF layers' neurons\n",
    "\n",
    "\n",
    "# Training (example parameters)\n",
    "lr = 0.001  # learning rate\n",
    "keep_prob = 0.8  # keep probability during dropout\n",
    "aux_loss_factor = 0.5  # weight of auxiliary loss term\n",
    "l2_prefactor = 10**(-5)  # prefactor for L2 regularization of weights\n",
    "\n",
    "# Specify the maximum number of stabilizer measurement cycles in training and validation datasets\n",
    "max_len_train_sequences = 40  # the maximum number of cycles in the training dataset\n",
    "max_len_validation_sequences = 10000  # the maximum number cycles in the validation dataset\n",
    "\n",
    "# Other hyperparameters\n",
    "batch_size_training = 64  # batch-size for training\n",
    "no_batches_feedback = 10  # number of batches for feedback\n",
    "batch_size_feedback = 100  # batch-size for feedback\n",
    "\n",
    "# # # TRAINING PIPELINE # # #\n",
    "\n",
    "# Parameters are as follows:\n",
    "# 1. minimum number of stabilizer measurement cycles of training sequences (None means all sequences in dataset can be used)\n",
    "# 2. maximum number of stabilizer measurement cycles of training sequences (None means all sequences in dataset can be used)\n",
    "# 3. number of epochs\n",
    "# 4. batches per epoch\n",
    "train_pipeline = []\n",
    "train_pipeline.append((1,  5, 10, 3000))\n",
    "train_pipeline.append((1, 10, 20, 5000))\n",
    "train_pipeline.append((1, 20, 30, 5000))\n",
    "train_pipeline.append((1, 30, 40, 5000))\n",
    "train_pipeline.append((None, None, 900, 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "added the following variables to L2 weight regularization term:\n",
      "NET/NET_FF/layer_0/weights:0\n",
      "NET/NET_FF/layer_1/weights:0\n",
      "NET/NET_FF_aux/layer_0/weights:0\n",
      "NET/NET_FF_aux/layer_1/weights:0\n"
     ]
    }
   ],
   "source": [
    "# Initialize decoder\n",
    "net = Decoder(code_distance=code_distance,\n",
    "              dim_syndr=dim_syndr,\n",
    "              dim_fsyndr=dim_fsyndr,\n",
    "              lstm_iss=[network_size, network_size],\n",
    "              ff_layer_sizes=[network_size],\n",
    "              checkpoint_path=\".\",\n",
    "              keep_prob=keep_prob,\n",
    "              aux_loss_factor=aux_loss_factor,\n",
    "              l2_prefactor=l2_prefactor\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Decoder at 0x149ae0910>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1 (main, Jan 21 2022, 17:32:03) [Clang 12.0.5 (clang-1205.0.22.11)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "667c0da509f68382cb5db87ad6084e99a8aa67a23a8521bc8a03c5d7e0fe7632"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
