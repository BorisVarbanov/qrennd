{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f46d6d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "from qrennd import get_model, Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02d083ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b520841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syndromes(anc_meas: xr.DataArray) -> xr.DataArray:\n",
    "    syndromes = anc_meas ^ anc_meas.shift(qec_round=1, fill_value=0)\n",
    "    syndromes.name = \"syndromes\"\n",
    "    return syndromes\n",
    "\n",
    "\n",
    "def get_defects(\n",
    "    syndromes: xr.DataArray, frame: Optional[xr.DataArray] = None\n",
    ") -> xr.DataArray:\n",
    "    shifted_syn = syndromes.shift(qec_round=1, fill_value=0)    \n",
    "    \n",
    "    if frame is not None:\n",
    "        shifted_syn[dict(qec_round = 0)] = frame\n",
    "    \n",
    "    defects = syndromes ^ shifted_syn\n",
    "    defects.name = \"defects\"\n",
    "    return defects\n",
    "\n",
    "def get_final_defects(\n",
    "    syndromes: xr.DataArray, proj_syndrome: xr.DataArray,\n",
    ") -> xr.DataArray:\n",
    "    last_syndrome = syndromes.isel(qec_round = -1)\n",
    "    proj_anc = proj_syndrome.anc_qubit\n",
    "    \n",
    "    final_defects = last_syndrome.sel(anc_qubit = proj_anc) ^ proj_syndrome\n",
    "    final_defects.name = \"final_defects\"\n",
    "    return final_defects\n",
    "\n",
    "def preprocess_data(dataset):\n",
    "    syndromes = get_syndromes(dataset.anc_meas)\n",
    "    defects = get_syndromes(dataset.anc_meas)\n",
    "\n",
    "    proj_syndrome = (dataset.data_meas @ PAR_MAT) % 2\n",
    "    final_defects = get_final_defects(syndromes, proj_syndrome)\n",
    "\n",
    "    init_states = dataset.init_state.sum(dim=\"data_qubit\") % 2\n",
    "    log_states = dataset.data_meas.sum(dim=\"data_qubit\") % 2\n",
    "\n",
    "    labels = log_states.astype(int) ^ init_states\n",
    "    \n",
    "    inputs = dict(\n",
    "        defects = defects.data,\n",
    "        final_defects = final_defects.data\n",
    "    )\n",
    "    outputs = labels.data\n",
    "    \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accf8eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAR_MAT = xr.DataArray(\n",
    "    data = [[1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "            [0, 1, 1, 0, 1, 1, 0, 0, 0],\n",
    "            [0, 0, 0, 1, 1, 0, 1, 1, 0],\n",
    "            [0, 0, 0, 0, 0, 1, 0, 0, 1]],\n",
    "    dims = [\"anc_qubit\", \"data_qubit\"],\n",
    "    coords = dict(\n",
    "        anc_qubit = [\"Z1\", \"Z2\", \"Z3\", \"Z4\"],\n",
    "        data_qubit = [\"D1\", \"D2\", \"D3\", \"D4\", \"D5\", \"D6\", \"D7\", \"D8\", \"D9\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553f97d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = pathlib.Path.cwd()\n",
    "\n",
    "CONFIG_DIR = NOTEBOOK_DIR.parent / \"configs\"\n",
    "if not CONFIG_DIR.exists():\n",
    "    raise ValueError(\"Config directory does not exist.\")\n",
    "    \n",
    "TRAIN_DATA_DIR = NOTEBOOK_DIR / \"data/train\"\n",
    "if not TRAIN_DATA_DIR.exists():\n",
    "    raise ValueError(\"Train data directory does not exist.\")\n",
    "    \n",
    "DEV_DATA_DIR = NOTEBOOK_DIR / \"data/dev\"\n",
    "if not DEV_DATA_DIR.exists():\n",
    "    raise ValueError(\"Dev data directory does not exist.\")\n",
    "    \n",
    "TEST_DATA_DIR = NOTEBOOK_DIR / \"data/test\"\n",
    "if not TEST_DATA_DIR.exists():\n",
    "    raise ValueError(\"Test data directory does not exist.\")\n",
    "    \n",
    "cur_datetime = datetime.now()\n",
    "datetime_str = cur_datetime.strftime('%Y%m%d-%H%M%S')\n",
    "LOG_DIR = NOTEBOOK_DIR / f\".logs/{datetime_str}\"\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7426c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config.from_yaml(CONFIG_DIR / \"base_config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28dcce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = xr.load_dataset(TRAIN_DATA_DIR / \"d3_surf_code_seq_round_state_0_shots_1000000_rounds_40.nc\")\n",
    "train_input, train_output = preprocess_data(train_dataset)\n",
    "\n",
    "dev_dataset = xr.load_dataset(DEV_DATA_DIR / \"d3_surf_code_seq_round_state_0_shots_20000_rounds_40.nc\")\n",
    "dev_input, dev_output = preprocess_data(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a75c4d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-01 16:35:47.587100: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = get_model(\n",
    "    syn_shape=train_input[\"defects\"].shape[1:],\n",
    "    proj_syn_shape=train_input[\"final_defects\"].shape[1:],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17613bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " defects (InputLayer)           [(None, 40, 8)]      0           []                               \n",
      "                                                                                                  \n",
      " LSTM_1 (LSTM)                  (None, 40, 32)       5248        ['defects[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_LSTM_1 (Dropout)       (None, 40, 32)       0           ['LSTM_1[0][0]']                 \n",
      "                                                                                                  \n",
      " LSTM_2 (LSTM)                  (None, 32)           8320        ['dropout_LSTM_1[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_LSTM_2 (Dropout)       (None, 32)           0           ['LSTM_2[0][0]']                 \n",
      "                                                                                                  \n",
      " relu (Activation)              (None, 32)           0           ['dropout_LSTM_2[0][0]']         \n",
      "                                                                                                  \n",
      " final_defects (InputLayer)     [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 36)           0           ['relu[0][0]',                   \n",
      "                                                                  'final_defects[0][0]']          \n",
      "                                                                                                  \n",
      " main_dense (Dense)             (None, 32)           1184        ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      " aux_dense (Dense)              (None, 32)           1056        ['relu[0][0]']                   \n",
      "                                                                                                  \n",
      " dropout_main_dense (Dropout)   (None, 32)           0           ['main_dense[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_aux_dense (Dropout)    (None, 32)           0           ['aux_dense[0][0]']              \n",
      "                                                                                                  \n",
      " main_output (Dense)            (None, 1)            33          ['dropout_main_dense[0][0]']     \n",
      "                                                                                                  \n",
      " aux_output (Dense)             (None, 1)            33          ['dropout_aux_dense[0][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 15,874\n",
      "Trainable params: 15,874\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87030b1",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ef66cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a62a239cd85c7540\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a62a239cd85c7540\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir={LOG_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a65597a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "15625/15625 [==============================] - 426s 27ms/step - loss: 0.7390 - main_output_loss: 0.4709 - aux_output_loss: 0.5334 - main_output_accuracy: 0.7345 - aux_output_accuracy: 0.7083 - val_loss: 0.5930 - val_main_output_loss: 0.3623 - val_aux_output_loss: 0.4582 - val_main_output_accuracy: 0.8099 - val_aux_output_accuracy: 0.7715\n",
      "Epoch 2/5\n",
      "15625/15625 [==============================] - 478s 31ms/step - loss: 0.5889 - main_output_loss: 0.3580 - aux_output_loss: 0.4584 - main_output_accuracy: 0.8224 - aux_output_accuracy: 0.7785 - val_loss: 0.5226 - val_main_output_loss: 0.3093 - val_aux_output_loss: 0.4235 - val_main_output_accuracy: 0.8509 - val_aux_output_accuracy: 0.8020\n",
      "Epoch 3/5\n",
      "15625/15625 [==============================] - 518s 33ms/step - loss: 0.5641 - main_output_loss: 0.3396 - aux_output_loss: 0.4459 - main_output_accuracy: 0.8357 - aux_output_accuracy: 0.7890 - val_loss: 0.4945 - val_main_output_loss: 0.2880 - val_aux_output_loss: 0.4100 - val_main_output_accuracy: 0.8661 - val_aux_output_accuracy: 0.8166\n",
      "Epoch 4/5\n",
      "15625/15625 [==============================] - 523s 34ms/step - loss: 0.5180 - main_output_loss: 0.3050 - aux_output_loss: 0.4229 - main_output_accuracy: 0.8604 - aux_output_accuracy: 0.8088 - val_loss: 0.4843 - val_main_output_loss: 0.2800 - val_aux_output_loss: 0.4055 - val_main_output_accuracy: 0.8726 - val_aux_output_accuracy: 0.8187\n",
      "Epoch 5/5\n",
      "15625/15625 [==============================] - 539s 35ms/step - loss: 0.4922 - main_output_loss: 0.2854 - aux_output_loss: 0.4104 - main_output_accuracy: 0.8746 - aux_output_accuracy: 0.8198 - val_loss: 0.4517 - val_main_output_loss: 0.2559 - val_aux_output_loss: 0.3884 - val_main_output_accuracy: 0.8863 - val_aux_output_accuracy: 0.8307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15aadf610>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard = callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=1)\n",
    "\n",
    "model.fit(\n",
    "    x=train_input,\n",
    "    y=train_output, \n",
    "    validation_data=[dev_input, dev_output],\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    callbacks=[\n",
    "        tensorboard,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d15a6ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = xr.load_dataset(TEST_DATA_DIR / \"d3_surf_code_seq_round_state_0_shots_20000_rounds_20.nc\")\n",
    "test_input, test_output = preprocess_data(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a45821e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.2916 - main_output_loss: 0.1318 - aux_output_loss: 0.3165 - main_output_accuracy: 0.9521 - aux_output_accuracy: 0.8816\n"
     ]
    }
   ],
   "source": [
    "eval_output = model.evaluate(\n",
    "    x=test_input,\n",
    "    y=test_output,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10e1b39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = xr.load_dataset(TEST_DATA_DIR / \"d3_surf_code_seq_round_state_0_shots_20000_rounds_20_v2.nc\")\n",
    "test_input, test_output = preprocess_data(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "682f1b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0575 - main_output_loss: 0.0045 - aux_output_loss: 0.1029 - main_output_accuracy: 0.9994 - aux_output_accuracy: 0.9912\n"
     ]
    }
   ],
   "source": [
    "eval_output = model.evaluate(\n",
    "    x=test_input,\n",
    "    y=test_output,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b4e54332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import tee\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize as optim\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.compat.v1.nn.rnn_cell import (BasicLSTMCell, DropoutWrapper,\n",
    "                                              MultiRNNCell)\n",
    "\n",
    "\n",
    "def decay_single_param(t, p_logical):\n",
    "    \"\"\"Function that models exponential (fidelity) decay controlled\n",
    "        by a single parameter, the decay rate.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    t -- Time, a positive float or integer.\n",
    "    p_logical -- Decay rate, a positive float number.\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    fidelity -- The logical fidelity, a float number between 0 and 1.\n",
    "    \"\"\"\n",
    "    fidelity = (1 + (1 - 2 * p_logical) ** t) / 2.0\n",
    "    return fidelity\n",
    "\n",
    "\n",
    "def calc_plog(data, p0=(0.001,), bounds=((0.00001), (0.2))):\n",
    "    \"\"\"Function to calculate the logical error rate.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    data -- List of lists.\n",
    "    p0 -- A starting point for the fits.\n",
    "    bounds -- Boundaries for the fitting, tuple of tuples of floats.\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    plog -- The logical error rate, a float number.\n",
    "    \"\"\"\n",
    "\n",
    "    # It is possible that the batch does not contain fidelities\n",
    "    # for all steps, hence we need a list with all steps for which\n",
    "    # predictions exist (we call it 'steps').\n",
    "    steps, data_nonzero = [], []\n",
    "    fids = []\n",
    "\n",
    "    # In the following we assume that the first step is s = 1.\n",
    "    for s in range(1, len(data) + 1):\n",
    "        dat = data[s - 1]\n",
    "        if len(dat) != 0:\n",
    "            # Non-trivial data points\n",
    "            steps.append(s)\n",
    "            data_nonzero.append(dat)\n",
    "            # Fidelities\n",
    "            fids.append(np.mean(dat))\n",
    "\n",
    "    # We fit a decay curve to the non-tivial data.\n",
    "    popt, pcov = optim.curve_fit(decay_single_param, steps, fids, p0, bounds=bounds)\n",
    "    plog = popt[0]\n",
    "    return plog\n",
    "\n",
    "\n",
    "class Decoder:\n",
    "\n",
    "    \"\"\"This class describes a neural network which consist of several\n",
    "    long-short term memory (LSTM) layers followed by two heads consitings of\n",
    "    feed forward layers. It is designed for quantum error correction on\n",
    "    stabilizer codes, such as the surface codes or the color code.\n",
    "\n",
    "    The __init__ function takes the following parameters:\n",
    "\n",
    "    code_distance -- The distance of the quantum error correction code.\n",
    "\n",
    "    dim_syndr -- The dimension of the syndrome increments, i.e. the number\n",
    "                 of stabilizers. In a surface 17 code for example, it would be 8.\n",
    "\n",
    "    dim_fsyndr -- The dimension of the final syndrome increment (which one can\n",
    "                  calculate from the readout of the data qubits).\n",
    "\n",
    "    lstm_iss -- A list containing the sizes of the internal states of the\n",
    "                LSTM layers.\n",
    "\n",
    "    ff_layer_sizes -- A list containing the number of neurons per layer of\n",
    "                      the feedforward networks.\n",
    "\n",
    "    checkpoint_path -- A path to which the network gets saved to, both for\n",
    "                       intermediate and final instances.\n",
    "\n",
    "    keep_prob -- Dropout regularization of the feedforward and LSTM layers. This\n",
    "                 parameter controlls the percentage of how many entries of the\n",
    "                 output vectors are NOT set to zero (on average) during training.\n",
    "\n",
    "    aux_loss_factor -- This scalar parameters controls how much the cost function\n",
    "                       weighs the auxillary head of the network. If it is 1,\n",
    "                       then both heads are weighted equally.\n",
    "\n",
    "    l2_prefactor -- Prefactor for L2 weight regularization of the feed forward\n",
    "                    layers.\n",
    "    \"\"\"\n",
    "\n",
    "    # # # Initialization functions # # #\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        code_distance,\n",
    "        dim_syndr,\n",
    "        dim_fsyndr,\n",
    "        lstm_iss,\n",
    "        ff_layer_sizes,\n",
    "        checkpoint_path,\n",
    "        keep_prob=1,\n",
    "        aux_loss_factor=1,\n",
    "        l2_prefactor=0,\n",
    "    ):\n",
    "        \"\"\"This function initializes an instance of the Decoder class. Its inputs\n",
    "        are described in the class documentation above.\"\"\"\n",
    "\n",
    "        # Factor by which the loss of the auxillary head is multiplied\n",
    "        self.aux_loss_factor = aux_loss_factor\n",
    "\n",
    "        # Directory to save the network and feedback (must exist)\n",
    "        self.cp_path = checkpoint_path\n",
    "\n",
    "        # Initialize input related variables\n",
    "        self._init_data_params(code_distance, dim_syndr, dim_fsyndr)\n",
    "\n",
    "        # Set parameters that define the network size\n",
    "        self._init_network_params(lstm_iss, ff_layer_sizes)\n",
    "\n",
    "        # Set parameters that control the training\n",
    "        self._init_training_params(keep_prob, l2_prefactor)\n",
    "\n",
    "        # Build the graph\n",
    "        self._init_graph()\n",
    "\n",
    "    def _init_data_params(self, code_distance, dim_syndr, dim_fsyndr):\n",
    "        \"\"\"A subfunction of __init__, setting variables related to the input\n",
    "        data, the input variables are described in class documentation above.\n",
    "        \"\"\"\n",
    "        self.code_dist = code_distance\n",
    "        self.dim_syndr = dim_syndr\n",
    "        self.dim_fsyndr = dim_fsyndr\n",
    "\n",
    "    def _init_network_params(self, lstm_iss, ff_layer_sizes):\n",
    "        \"\"\"A subfunction of __init__, setting the variables that define the\n",
    "        network size, the input variables are described in class documentation\n",
    "        above.\"\"\"\n",
    "\n",
    "        # LSTM layers' parameters\n",
    "        self.lstm_iss = lstm_iss\n",
    "\n",
    "        # Feedfoward layers' parameters\n",
    "        self.ff_aux_lr_s = [lstm_iss[-1]] + ff_layer_sizes + [1]\n",
    "        self.ff_lr_s = [lstm_iss[-1] + self.dim_fsyndr] + ff_layer_sizes + [1]\n",
    "\n",
    "    def _init_training_params(self, keep_prob, l2_prefactor):\n",
    "        \"\"\"A subfunction of __init__, setting the variables that define the\n",
    "        training procedure, the input variables are described in class\n",
    "        documentation above.\"\"\"\n",
    "\n",
    "        # Dropout of the outputs in the LSTM network\n",
    "        self.kp = keep_prob\n",
    "\n",
    "        # Prefactor for L2 weight regularization (feedforward layers only)\n",
    "        self.l2_prefact = l2_prefactor\n",
    "\n",
    "        # Variables to keep track of training process\n",
    "        self.total_trained_epochs = 0\n",
    "        self.total_trained_batches = 0\n",
    "\n",
    "    def _init_graph(self):\n",
    "        \"\"\"A subfunction of __init__, defining the graph, initializing the\n",
    "        tensorflow variables, and the optimizer.\"\"\"\n",
    "\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self._init_network_variables()\n",
    "            self._init_network_functions()\n",
    "\n",
    "    def _init_network_variables(self):\n",
    "        \"\"\"A subfunction of _init_graph, defining the tensorflow placeholders,\n",
    "        weights and biases of the feed forward networks, and corresponding\n",
    "        saver instances.\"\"\"\n",
    "\n",
    "        # Here we defind placeholders ...\n",
    "        with tf.variable_scope(\"input\"):\n",
    "            # ... for the input of the syndrome increments\n",
    "            self.x = tf.placeholder(\n",
    "                tf.float32, [None, None, self.dim_syndr], name=\"x_input\"\n",
    "            )\n",
    "            # ... for the input of the final syndrome increments\n",
    "            self.fx = tf.placeholder(\n",
    "                tf.float32, [None, self.dim_fsyndr], name=\"fx_input\"\n",
    "            )\n",
    "            # ... for the parity of the bitflips\n",
    "            self.y = tf.placeholder(tf.float32, [None, 1], name=\"y_input\")\n",
    "            # ... for the number of stabilizer measurement cycles in a sequence\n",
    "            self.length = tf.placeholder(tf.int32, [None], name=\"length_input\")\n",
    "\n",
    "        with tf.variable_scope(\"training_parameters\"):\n",
    "            # ... for the learning rate\n",
    "            self.lr = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "            # ... for the weighing of the auxillary head\n",
    "            self.alf = tf.placeholder(tf.float32, name=\"aux_loss_factor\")\n",
    "\n",
    "            # ... for the dropout (keep probabilities)\n",
    "            self.lstm_kp = tf.placeholder(tf.float32, name=\"lstm_keep_probability\")\n",
    "            self.ff_kp = tf.placeholder(tf.float32, name=\"ff_keep_probability\")\n",
    "\n",
    "        with tf.variable_scope(\"summary_placeholders\"):\n",
    "            # ... for the tensorboard summaries\n",
    "            self.plog = tf.placeholder(tf.float32, name=\"plog_train\")\n",
    "            self.plog_aux = tf.placeholder(tf.float32, name=\"plog_aux_train\")\n",
    "            self.tot_cost = tf.placeholder(tf.float32, name=\"tot_cost\")\n",
    "\n",
    "    def _init_network_functions(self):\n",
    "        \"\"\"A subfunction of _init_graph, defining all the main functions of the\n",
    "        graph.\"\"\"\n",
    "\n",
    "        with tf.variable_scope(\"NET\"):\n",
    "            # Gathering the network outputs (logits)\n",
    "            out, out_aux = self.network(self.x, self.fx)\n",
    "            logits = tf.reshape(out, [-1, 1])\n",
    "            logits_aux = tf.reshape(out_aux, [-1, 1])\n",
    "\n",
    "        with tf.variable_scope(\"prediction\"):\n",
    "            # Calculating the probabilities that the parity of bitflips is odd\n",
    "            self.predictions = tf.nn.sigmoid(logits)\n",
    "            self.predictions_aux = tf.nn.sigmoid(logits_aux)\n",
    "            p = tf.nn.sigmoid(logits)\n",
    "            p_aux = tf.nn.sigmoid(logits_aux)\n",
    "\n",
    "            # Adding the network outputs and predictions to the summary\n",
    "            tf.summary.histogram(\"logits\", clip(logits), collections=[\"feedback\"])\n",
    "            tf.summary.histogram(\"p\", p, collections=[\"feedback\"])\n",
    "            tf.summary.histogram(\n",
    "                \"logits_aux\", clip(logits_aux), collections=[\"feedback\"]\n",
    "            )\n",
    "            tf.summary.histogram(\"p_aux\", p_aux, collections=[\"feedback\"])\n",
    "\n",
    "        with tf.variable_scope(\"cost\"):\n",
    "            # Calculate the cross entropy for the main head\n",
    "            cross_entropy = tf.losses.sigmoid_cross_entropy(\n",
    "                logits=logits, multi_class_labels=self.y\n",
    "            )\n",
    "            self.cost_crossentro = tf.reduce_sum(cross_entropy)\n",
    "\n",
    "            # Calculate the cross entropy for the auxillary head\n",
    "            cross_entropy_aux = tf.losses.sigmoid_cross_entropy(\n",
    "                logits=logits_aux, multi_class_labels=self.y\n",
    "            )\n",
    "            self.cost_crossentro_aux = tf.reduce_sum(cross_entropy_aux)\n",
    "\n",
    "            # Calculate the L2 norm of the feed forward networks' weights\n",
    "            # to do weight regularization (not for the biases)\n",
    "            col_ff = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"NET/NET_FF\")\n",
    "            weights_l = []\n",
    "            for el in col_ff:\n",
    "                if \"weights\" in el.name:\n",
    "                    weights_l.append(el)\n",
    "            self.l2_loss = (\n",
    "                tf.add_n([tf.nn.l2_loss(v) for v in weights_l]) * self.l2_prefact\n",
    "            )\n",
    "            # Print some feedback\n",
    "            print(\"added the following variables to L2 weight regularization term:\")\n",
    "            for var in weights_l:\n",
    "                print(var.name)\n",
    "\n",
    "            # The total cost function is the sum of the two cross-entropies plus\n",
    "            # the weight regularization term.\n",
    "            self.cost = (\n",
    "                self.cost_crossentro\n",
    "                + self.l2_loss\n",
    "                + self.aux_loss_factor * self.cost_crossentro_aux\n",
    "            )\n",
    "\n",
    "        # Writing the costs and logical error rates to the feedback summary.\n",
    "        with tf.variable_scope(\"feedback\"):\n",
    "            # costs\n",
    "            tf.summary.scalar(\n",
    "                \"crossentropy\", self.cost_crossentro, collections=[\"feedback\"]\n",
    "            )\n",
    "            tf.summary.scalar(\n",
    "                \"crossentropy_aux\", self.cost_crossentro_aux, collections=[\"feedback\"]\n",
    "            )\n",
    "            tf.summary.scalar(\"l2_loss\", self.l2_loss, collections=[\"feedback\"])\n",
    "            tf.summary.scalar(\"cost\", self.tot_cost, collections=[\"feedback\"])\n",
    "\n",
    "            # logical error rate\n",
    "            tf.summary.scalar(\"logical_error_rate\", self.plog, collections=[\"feedback\"])\n",
    "            tf.summary.scalar(\n",
    "                \"logical_error_rate_aux\", self.plog_aux, collections=[\"feedback\"]\n",
    "            )\n",
    "\n",
    "        # Writing some feedback regarding the nature of the input data\n",
    "        with tf.variable_scope(\"network_parameters\"):\n",
    "            tf.summary.scalar(\n",
    "                \"min_length\", tf.reduce_min(self.length), collections=[\"feedback\"]\n",
    "            )\n",
    "            tf.summary.scalar(\n",
    "                \"max_length\", tf.reduce_max(self.length), collections=[\"feedback\"]\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope(\"optimizer\"):\n",
    "            # Defining the network optimization algorithm\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(\n",
    "                self.cost\n",
    "            )\n",
    "\n",
    "        # Tensorflow saver, to save checkpoints of the network\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # Merge summaries\n",
    "        self.merged_summaries = tf.summary.merge_all(\"network\")\n",
    "        self.merged_summaries_fb = tf.summary.merge_all(\"feedback\")\n",
    "\n",
    "        # Define separate summary writers for training and validation\n",
    "        self.train_writer = tf.summary.FileWriter(\n",
    "            self.cp_path + \"/tensorboard/training\", self.graph\n",
    "        )\n",
    "        self.val_writer = tf.summary.FileWriter(\n",
    "            self.cp_path + \"/tensorboard/validation\", self.graph\n",
    "        )\n",
    "\n",
    "        # Finally, we initialize the network variables\n",
    "        self.initialize_NN = tf.global_variables_initializer()\n",
    "\n",
    "    # # # Functions that define the neural network # # #\n",
    "    def network(self, input_syndr, input_fsyndr):\n",
    "        \"\"\"This function defines the neural network.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        input_syndr -- A placeholder that will later contain the lists with\n",
    "                       syndrome increments.\n",
    "\n",
    "        input_fsyndr -- A placeholder that will later contain the final\n",
    "                        syndrome increments.\n",
    "\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        ff_net -- The neural network with the main evaluation head.\n",
    "\n",
    "        ff_net_aux -- The neural network with the auxillary evaluation head.\n",
    "        \"\"\"\n",
    "\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "        # # # # # # # #  Recurrent part (LSTM) of the network # # # # # # # # #\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "        with tf.variable_scope(\"NET_LSTM\"):\n",
    "            # First, we create LSTM cells with (optional) dropout. Here dropout is\n",
    "            # only applied to the outputs of the layers.\n",
    "            cells = []\n",
    "            for iss in self.lstm_iss:\n",
    "                cell = BasicLSTMCell(iss)\n",
    "                cell = DropoutWrapper(cell=cell, output_keep_prob=self.lstm_kp)\n",
    "                cells.append(cell)\n",
    "\n",
    "            # Next, we combine LSTM cells into a recurrent network.\n",
    "            lstm_cells = MultiRNNCell(cells)\n",
    "            # Since each batch will contain sequences of variying number or\n",
    "            # measurement cycles, we use dynamic_rrn which allows for a variable\n",
    "            # sequence_length.\n",
    "            lstm_out, lstm_states = tf.nn.dynamic_rnn(\n",
    "                lstm_cells,\n",
    "                input_syndr,\n",
    "                sequence_length=self.length,\n",
    "                dtype=tf.float32,\n",
    "                scope=\"dyn_rnn\",\n",
    "            )\n",
    "\n",
    "            # We now select the output of the last LSTM cell after the last cycle, as\n",
    "            # specified by self.length.\n",
    "            last_lstm_out = tf.gather_nd(\n",
    "                lstm_out,\n",
    "                tf.stack([tf.range(tf.shape(lstm_out)[0]), self.length - 1], axis=1),\n",
    "            )\n",
    "            # We store this last output as feedback to be displayed with tensorboard.\n",
    "            tf.summary.histogram(\n",
    "                \"lstm_out\", clip(last_lstm_out), collections=[\"feedback\"]\n",
    "            )\n",
    "            # Finally, we apply a rectified linear activation function to the output.\n",
    "            last_lstm_out = tf.nn.relu(last_lstm_out)\n",
    "\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "        # # # # # # # # # Feedforward part of the network # # # # # # # # # # #\n",
    "        # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n",
    "\n",
    "        # # # Main evaluation head # # #\n",
    "        with tf.variable_scope(\"NET_FF\"):\n",
    "            # The main evaluation head is a feedforward network with rectified linear\n",
    "            # units. It gets as an input last_lstm_out concatenated with the final\n",
    "            # syndrome increment.\n",
    "            ff_net = tf.concat([last_lstm_out, input_fsyndr], 1)\n",
    "\n",
    "            # Here we define the feedforward network. The output will be a\n",
    "            # single float number. The final output is not subject to an\n",
    "            # activation function.\n",
    "            n_lrs = len(self.ff_lr_s) - 1\n",
    "            for n in range(n_lrs):\n",
    "                if n < n_lrs - 1:\n",
    "                    act_fct = tf.nn.relu\n",
    "                    has_bias = True\n",
    "                else:\n",
    "                    act_fct = tf.identity\n",
    "                    has_bias = False\n",
    "                ff_net = self._make_layer(\n",
    "                    ff_net,\n",
    "                    self.ff_lr_s[n],\n",
    "                    self.ff_lr_s[n + 1],\n",
    "                    act_fct,\n",
    "                    \"layer_\" + str(n),\n",
    "                    has_bias,\n",
    "                )\n",
    "\n",
    "        # # # Auxillary evaluation head # # #\n",
    "        with tf.variable_scope(\"NET_FF_aux\"):\n",
    "            # The auxillary evaluation head is a feedforward network with rectified\n",
    "            # linear units. It gets as an input last_lstm_out. Its purpose is to\n",
    "            # encourage translation invariance in time of the recurrent network.\n",
    "            ff_net_aux = last_lstm_out\n",
    "\n",
    "            # Here we define the feedforward network. The output will be a\n",
    "            # single float number. The final output is not subject to an\n",
    "            # activation function.\n",
    "            n_lrs = len(self.ff_aux_lr_s) - 1\n",
    "            for n in range(n_lrs):\n",
    "                if n < n_lrs - 1:\n",
    "                    act_fct = tf.nn.relu\n",
    "                    has_bias = True\n",
    "                else:\n",
    "                    act_fct = tf.identity\n",
    "                    has_bias = False\n",
    "                ff_net_aux = self._make_layer(\n",
    "                    ff_net_aux,\n",
    "                    self.ff_aux_lr_s[n],\n",
    "                    self.ff_aux_lr_s[n + 1],\n",
    "                    act_fct,\n",
    "                    \"layer_\" + str(n),\n",
    "                    has_bias,\n",
    "                )\n",
    "\n",
    "        return ff_net, ff_net_aux\n",
    "\n",
    "    def _make_layer(self, input_tensor, dim_in, dim_out, act_fct, name, has_bias=True):\n",
    "        \"\"\"This function builds a single layer of a feedforward network.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        input_tensor -- The tensor that will be fed into the layer.\n",
    "\n",
    "        dim_in -- The dimension of the input tensor.\n",
    "\n",
    "        dim_out -- The number of neurons in the layer.\n",
    "\n",
    "        act_fct -- The activation function.\n",
    "\n",
    "        name -- The name of the layer.\n",
    "\n",
    "        has_bias -- If True, the layer will have a bias vector added to\n",
    "                    the neurons (before the activation function is applied).\n",
    "\n",
    "\n",
    "        Output\n",
    "        ------\n",
    "        layer_after_dropout -- The layer with dropout.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        with tf.variable_scope(name):\n",
    "            # First, initialize the weights and biases\n",
    "            weights = tf.Variable(\n",
    "                tf.random_uniform(\n",
    "                    [dim_in, dim_out],\n",
    "                    minval=-1.0 / np.sqrt(dim_in),\n",
    "                    maxval=+1.0 / np.sqrt(dim_in),\n",
    "                ),\n",
    "                dtype=tf.float32,\n",
    "                name=\"weights\",\n",
    "            )\n",
    "            tf.summary.histogram(\"weights\", clip(weights), collections=[\"network\"])\n",
    "            if has_bias:\n",
    "                biases = tf.Variable(\n",
    "                    tf.constant(0.0, tf.float32, [dim_out]),\n",
    "                    dtype=tf.float32,\n",
    "                    name=\"biases\",\n",
    "                )\n",
    "                tf.summary.histogram(\"biases\", clip(biases), collections=[\"network\"])\n",
    "            # Second, define the layer.\n",
    "            if has_bias:\n",
    "                layer = tf.matmul(input_tensor, weights) + biases\n",
    "            else:\n",
    "                layer = tf.matmul(input_tensor, weights)\n",
    "            # ... and write some feedback\n",
    "            tf.summary.histogram(\"preact\", clip(layer), collections=[\"feedback\"])\n",
    "\n",
    "            # Third, apply the activation function\n",
    "            layer_activations = act_fct(layer, name=\"activations\")\n",
    "            # ... and write some feedback\n",
    "            tf.summary.histogram(\n",
    "                \"act\", clip(layer_activations), collections=[\"feedback\"]\n",
    "            )\n",
    "\n",
    "            # Finally, apply dropout to the output of the layer\n",
    "            layer_after_dropout = tf.nn.dropout(layer_activations, self.ff_kp)\n",
    "        return layer_after_dropout\n",
    "\n",
    "    # # # Starting and ending the session # # #\n",
    "    def start_session(self, gpu_options=None):\n",
    "        \"\"\"This function (re-)initializes the TensorFlow session\"\"\"\n",
    "\n",
    "        # Optionally, GPU options can be specified.\n",
    "        if gpu_options is None:\n",
    "            self.sess = tf.Session(graph=self.graph)\n",
    "        else:\n",
    "            self.sess = tf.Session(\n",
    "                graph=self.graph, config=tf.ConfigProto(gpu_options=gpu_options)\n",
    "            )\n",
    "\n",
    "        # Initialize the TensorFlow session.\n",
    "        self.sess.run(self.initialize_NN)\n",
    "\n",
    "    def end_session(self):\n",
    "        \"\"\"This function ends the current TensorFlow session\"\"\"\n",
    "        self.sess.close()\n",
    "\n",
    "    def feed_dict(self, b_x, b_fx, b_l, b_y, learning_rate, lstm_kp, ff_kp):\n",
    "        \"\"\"This function creates and returns a 'feed dictionary', that\n",
    "        can be passed to the session for network training and evaluation\n",
    "        purposes\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        b_x -- A batch with syndrome increments.\n",
    "\n",
    "        b_fx -- A batch with final syndrome increments.\n",
    "\n",
    "        b_l -- A batch specifying the number of stabilizer measurement\n",
    "               cycles for each batch entry.\n",
    "\n",
    "        b_y -- A batch with the true parities.\n",
    "\n",
    "        learning_rate -- A single float number which sets the learning\n",
    "                         rate of the optimizer.\n",
    "\n",
    "        lstm_kp -- A single float number that sets the keep-rate of the\n",
    "                   dropout after the LSTM layers.\n",
    "\n",
    "        ff_kp -- A single float number that sets the keep-rate of the\n",
    "                 dropout after the feedforward layers.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            self.x: b_x,\n",
    "            self.fx: b_fx,\n",
    "            self.length: b_l,\n",
    "            self.y: b_y,\n",
    "            self.lr: learning_rate,\n",
    "            self.lstm_kp: lstm_kp,\n",
    "            self.ff_kp: ff_kp,\n",
    "        }\n",
    "\n",
    "    # # # Functions for training the network # # #\n",
    "    def train_one_epoch(self, train_batches, learning_rate=0.001):\n",
    "        \"\"\"This function controlls the training of the network for one epoch.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        train_batches -- The batches that consitute one epoch.\n",
    "\n",
    "        learning_rate -- The learning rate of the (Adam) optimizer.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Here we train the network\n",
    "        for b_s, b_x, b_fx, b_l, b_y in train_batches:\n",
    "            fd = self.feed_dict(b_x, b_fx, b_l, b_y, learning_rate, self.kp, self.kp)\n",
    "            self.sess.run(self.optimizer, feed_dict=fd)\n",
    "            self.total_trained_batches += 1\n",
    "        self.total_trained_epochs += 1\n",
    "\n",
    "        # After the training we write some feedback to the summaries\n",
    "        summary = self.sess.run(self.merged_summaries, feed_dict=fd)\n",
    "        self.train_writer.add_summary(summary, self.total_trained_epochs)\n",
    "\n",
    "        # Finally, we save the network\n",
    "        self.save_network(\"model\")\n",
    "\n",
    "    def calc_feedback(self, batches, validation=True):\n",
    "        \"\"\"This function calculates feedback and summaries.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        batches -- The batches over which the feedback is calculated.\n",
    "\n",
    "        validation -- If True, the validation summary writer is used.\n",
    "                      If False, the traning summary writer is used.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # First, we copy the batch generator two times\n",
    "        batches, batches_copy = tee(batches)\n",
    "        batches, batches_copy_aux = tee(batches)\n",
    "\n",
    "        # We use the first copy to calculate the logical error rate for\n",
    "        # the main head ...\n",
    "        plog = calc_plog(self.test_net(batches_copy, auxillary=False))\n",
    "        # ... and the second copy for the auxillary head\n",
    "        plog_aux = calc_plog(self.test_net(batches_copy_aux, auxillary=True))\n",
    "\n",
    "        # We then use the original generator to calculate the cost.\n",
    "        # For practicality, we only use a single batch for that and\n",
    "        # the other feedback\n",
    "        b_s, b_x, b_fx, b_l, b_y = next(batches)\n",
    "        fd = self.feed_dict(b_x, b_fx, b_l, b_y, 0, 1, 1)\n",
    "        cost = self.cost.eval(session=self.sess, feed_dict=fd)\n",
    "\n",
    "        # Finally, we write this information to the summary\n",
    "        fd = {\n",
    "            self.plog: plog,\n",
    "            self.plog_aux: plog_aux,\n",
    "            self.tot_cost: cost,\n",
    "            self.x: b_x,\n",
    "            self.fx: b_fx,\n",
    "            self.length: b_l,\n",
    "            self.y: b_y,\n",
    "            self.lr: 0,\n",
    "            self.lstm_kp: 1,\n",
    "            self.ff_kp: 1,\n",
    "        }\n",
    "        summary_fb = self.sess.run(self.merged_summaries_fb, feed_dict=fd)\n",
    "        if validation:\n",
    "            self.val_writer.add_summary(summary_fb, self.total_trained_epochs)\n",
    "            print(\"logical error rate on validation set is\", round(plog, 4))\n",
    "        else:\n",
    "            self.train_writer.add_summary(summary_fb, self.total_trained_epochs)\n",
    "            print(\"logical error rate on training set is\", round(plog, 4))\n",
    "        return plog\n",
    "\n",
    "    def test_net(self, batches, auxillary=False):\n",
    "        \"\"\"A function that evaluates the network.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        batches -- A generator of batches that will be evaluated.\n",
    "\n",
    "        auxillary -- If True, the auxillary head is used for evaluation.\n",
    "                     If False, the main head is used for evaluation.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        length_list, comp_list = [], []\n",
    "        for b_s, b_x, b_fx, b_l, b_y in batches:\n",
    "            # Get probabilities that a bitflip occured from the network\n",
    "            fd = self.feed_dict(b_x, b_fx, b_l, b_y, 0, 1, 1)\n",
    "            if auxillary:\n",
    "                preds = self.predictions_aux.eval(feed_dict=fd, session=self.sess)\n",
    "            else:\n",
    "                preds = self.predictions.eval(feed_dict=fd, session=self.sess)\n",
    "\n",
    "            # Reshape tensors to 1D\n",
    "            preds = np.reshape(preds, [-1])\n",
    "            b_y = np.reshape(b_y, [-1])\n",
    "\n",
    "            # Make a decision (0 for even, 1 for odd) of the total parity of bitflips\n",
    "            preds = np.around(preds).astype(bool)\n",
    "\n",
    "            # Compare predictions to true results\n",
    "            comp_list += list(np.equal(preds, b_y))\n",
    "\n",
    "            # Update maximum length of time-sequence\n",
    "            length_list += list(b_l)\n",
    "\n",
    "        # Reshape into a list of lists\n",
    "        comparison = [[] for _ in range(max(length_list))]\n",
    "        for n in range(len(comp_list)):\n",
    "            idx = length_list[n] - 1\n",
    "            comparison[idx].append(comp_list[n])\n",
    "\n",
    "        return comparison\n",
    "\n",
    "    # # # Functions to save and restore the network # # #\n",
    "    def save_network(self, fname, with_step=True):\n",
    "        \"\"\"Function to save all variables of the network.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        fname -- The  filename to which the network is saved, note\n",
    "                 that by default the path is the checkpoint_path/model/.\n",
    "\n",
    "        with_step -- If True, the variable \"global_step\" will be set to\n",
    "                     self.total_trained_epochs. This can be useful if the\n",
    "                     training has been interrupted.\n",
    "        \"\"\"\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            saver = tf.train.Saver()\n",
    "            if with_step:\n",
    "                saver.save(\n",
    "                    self.sess,\n",
    "                    self.cp_path + \"model/\" + fname,\n",
    "                    global_step=self.total_trained_epochs,\n",
    "                )\n",
    "            else:\n",
    "                saver.save(self.sess, self.cp_path + \"model/\" + fname)\n",
    "\n",
    "    def load_network(self, fname):\n",
    "        \"\"\"Function to load the network from a checkpoint.\n",
    "\n",
    "        Input\n",
    "        -----\n",
    "        fname -- is the filename from which the network is loaded, note\n",
    "                 that by default the path is the checkpoint_path/model/.\n",
    "\n",
    "        \"\"\"\n",
    "        with self.graph.as_default():\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(self.sess, self.cp_path + \"model/\" + fname)\n",
    "\n",
    "\n",
    "# # # Functions # # #\n",
    "\n",
    "\n",
    "def clip(var):\n",
    "    \"\"\"Clips elements of a tensor at mean +- 3 standard deviations.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    var -- A tensorflow tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    lin = tf.reshape(var, [-1])\n",
    "    mean, variance = tf.nn.moments(lin, axes=[0])\n",
    "    sigma = tf.sqrt(variance)\n",
    "    cv_min = mean - 3 * sigma\n",
    "    cv_max = mean + 3 * sigma\n",
    "    return tf.clip_by_value(var, cv_min, cv_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c7a80136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # PARAMETERS # # #\n",
    "\n",
    "# Network (parameters for 6-6-6 color code with distance 3)\n",
    "code_distance = 3  # distance of the QEC code\n",
    "dim_syndr = 12  # dimension of the syndrome increment vectors\n",
    "dim_fsyndr = 3  # dimension of the final syndrome increment vectors\n",
    "network_size = 32  # size of the LSTM's internal states and number of the the FF layers' neurons\n",
    "\n",
    "# Network (parameters for 6-6-6 color code with distance 5)\n",
    "# code_distance = 5  # distance of the QEC code\n",
    "# dim_syndr = 36  # dimension of the syndrome increment vectors\n",
    "# dim_fsyndr = 9  # dimension of the final syndrome increment vectors\n",
    "# network_size = 64  # size of the LSTM's internal states and number of the the FF layers' neurons\n",
    "\n",
    "# Network (parameters for 6-6-6 color code with distance 7)\n",
    "# code_distance = 7  # distance of the QEC code\n",
    "# dim_syndr = 72  # dimension of the syndrome increment vectors\n",
    "# dim_fsyndr = 18  # dimension of the final syndrome increment vectors\n",
    "# network_size = 128  # size of the LSTM's internal states and number of the the FF layers' neurons\n",
    "\n",
    "\n",
    "# Training (example parameters)\n",
    "lr = 0.001  # learning rate\n",
    "keep_prob = 0.8  # keep probability during dropout\n",
    "aux_loss_factor = 0.5  # weight of auxiliary loss term\n",
    "l2_prefactor = 10**(-5)  # prefactor for L2 regularization of weights\n",
    "\n",
    "# Specify the maximum number of stabilizer measurement cycles in training and validation datasets\n",
    "max_len_train_sequences = 40  # the maximum number of cycles in the training dataset\n",
    "max_len_validation_sequences = 10000  # the maximum number cycles in the validation dataset\n",
    "\n",
    "# Other hyperparameters\n",
    "batch_size_training = 64  # batch-size for training\n",
    "no_batches_feedback = 10  # number of batches for feedback\n",
    "batch_size_feedback = 100  # batch-size for feedback\n",
    "\n",
    "# # # TRAINING PIPELINE # # #\n",
    "\n",
    "# Parameters are as follows:\n",
    "# 1. minimum number of stabilizer measurement cycles of training sequences (None means all sequences in dataset can be used)\n",
    "# 2. maximum number of stabilizer measurement cycles of training sequences (None means all sequences in dataset can be used)\n",
    "# 3. number of epochs\n",
    "# 4. batches per epoch\n",
    "train_pipeline = []\n",
    "train_pipeline.append((1,  5, 10, 3000))\n",
    "train_pipeline.append((1, 10, 20, 5000))\n",
    "train_pipeline.append((1, 20, 30, 5000))\n",
    "train_pipeline.append((1, 30, 40, 5000))\n",
    "train_pipeline.append((None, None, 900, 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "40339bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/boris/Projects/decoders/qrennd/.venv/lib/python3.10/site-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:711: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
      "/Users/boris/Projects/decoders/qrennd/.venv/lib/python3.10/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1702: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added the following variables to L2 weight regularization term:\n",
      "NET/NET_FF/layer_0/weights:0\n",
      "NET/NET_FF/layer_1/weights:0\n",
      "NET/NET_FF_aux/layer_0/weights:0\n",
      "NET/NET_FF_aux/layer_1/weights:0\n"
     ]
    }
   ],
   "source": [
    "# Initialize decoder\n",
    "net = Decoder(code_distance=code_distance,\n",
    "              dim_syndr=dim_syndr,\n",
    "              dim_fsyndr=dim_fsyndr,\n",
    "              lstm_iss=[network_size, network_size],\n",
    "              ff_layer_sizes=[network_size],\n",
    "              checkpoint_path=\".\",\n",
    "              keep_prob=keep_prob,\n",
    "              aux_loss_factor=aux_loss_factor,\n",
    "              l2_prefactor=l2_prefactor\n",
    "             )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qrennd_kernel",
   "language": "python",
   "name": "qrennd_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
